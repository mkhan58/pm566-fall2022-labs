---
title: "lab07"
author: "Misha Khan"
date: "2022-10-05"
output: html_document
---

https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(httr)
library(tidyverse)
library(stringr)
```

Q1: What is the number of results on Sars-cov-2?
```{r}
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2")

  
# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")

# Turning it into text
counts <- as.character(counts)

# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")
```

Q2: You need to query the following The parameters passed to the query are documented here.
Use the function httr::GET() to make the following query:
Baseline URL: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi

Query parameters:
db: pubmed
term: covid19 hawaii
retmax: 1000
```{r}
library(httr)
query_ids <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query = list(
    db = "pubmed",
    term = "covid19 hawaii",
    retmax = 1000)
)

# Extracting the content of the response of GET
ids <- httr::content(query_ids)
```

Q3: Details about article
The Ids are wrapped around text in the following way: <Id>... id number ...</Id>. we can use a regular expression that extract that information. 
```{r}
# Turn the result into a character vector
ids <- as.character(ids)

# Find all the ids 
ids <- stringr::str_extract_all(ids, "<Id>[[:digit:]]+</Id>")[[1]]

# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "</?Id>")
#? means i do or dont find the /
head(ids)
```

With the ids in hand, we can now try to get the abstracts of the papers. As before, we will need to coerce the contents (results) to a list using:

Baseline url: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi

Query parameters:

db: pubmed
id: A character with all the ids separated by comma, e.g., “1232131,546464,13131”
retmax: 1000
rettype: abstract
Pro-tip: If you want GET() to take some element literal, wrap it around I() (as you would do in a formula in R). For example, the text "123,456" is replaced with "123%2C456". If you don’t want that behavior, you would need to do the following I("123,456").

Grab publications with Pudmed IDs
```{r}
publications <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
  query = list(
    db = "pubmed",
    id = paste(ids, collapse = ","),
    retmax = 1000,
    rettype = "abstract"
    )
)

# Turning the output into character vector
publications <- httr::content(publications)
#publications_txt <- as.character(publications)
```

Q5: Form a database
We want to build a dataset which includes the title and the abstract of the paper. The title of all records is enclosed by the HTML tag ArticleTitle, and the abstract by Abstract.
```{r}
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
```

Now, extract the abstract and article title for each one of the elements of pub_char_list.
```{r}
abstracts <- str_extract(pub_char_list, "<Abstract>[[:print:][:space:]]+</Abstract>")
abstracts[[1]]
abstracts <- str_remove_all(abstracts, "</?[[:alnum:]- =\"]+>") 
abstracts[[1]]
abstracts <- str_replace_all(abstracts, "[[:space:]]+"," ")
abstracts[[1]]
```

Now get the titles:
```{r}
titles <- str_extract(pub_char_list, "<ArticleTitle>[[:print:][:space:]]+</ArticleTitle>")
titles[[1]]
titles <- str_remove_all(titles, "</?[[:alnum:]- =\"]+>")
titles[[1]]
```

Finally the dataset:
```{r}
database <- data.frame(
  PubMedId = ids,
  Title    = titles,
  Abstract = abstracts
)
knitr::kable(database[1:8,], caption = "Some papers about Covid19 and Hawaii")
```

